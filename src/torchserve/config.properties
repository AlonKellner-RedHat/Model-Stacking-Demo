# TorchServe Configuration
# See: https://pytorch.org/serve/configuration.html

# Server settings
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Worker settings
# Use 1 worker per model to avoid memory issues with large models
default_workers_per_model=1

# Response timeout (seconds)
# EfficientDet inference can take ~100ms on CPU, set generous timeout
default_response_timeout=120

# Model loading
load_models=all
model_store=model_store

# Logging
async_logging=true

# Memory settings
# Disable JIT profiling to reduce memory overhead
disable_jit_optimization=false

# Batch settings (disabled by default, enable for throughput optimization)
# batch_size=1
# max_batch_delay=100

# Device settings
# Uncomment to force CPU
# number_of_gpu=0

# TorchServe Configuration
# See: https://pytorch.org/serve/configuration.html

# Server settings
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Worker settings
# Use 1 worker per model to avoid memory issues with large models
default_workers_per_model=1

# Response timeout (seconds)
# EfficientDet inference can take ~100ms on CPU, set generous timeout
default_response_timeout=120

# Model loading
load_models=all
model_store=model_store

# Logging
async_logging=true

# Memory settings
# Disable JIT profiling to reduce memory overhead
disable_jit_optimization=false

# Batch settings for dynamic batching
# TorchServe accumulates requests up to batch_size or max_batch_delay ms
# This is CRITICAL for throughput under concurrent load
batch_size=4
max_batch_delay=50

# Device settings
# Uncomment to force CPU
# number_of_gpu=0
